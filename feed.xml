<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://leerang77.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://leerang77.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-29T01:43:38+00:00</updated><id>https://leerang77.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Fine-tuning protein language model with Huggingface (Part 2)</title><link href="https://leerang77.github.io/blog/2025/foundation-model2/" rel="alternate" type="text/html" title="Fine-tuning protein language model with Huggingface (Part 2)"/><published>2025-07-27T15:09:00+00:00</published><updated>2025-07-27T15:09:00+00:00</updated><id>https://leerang77.github.io/blog/2025/foundation-model2</id><content type="html" xml:base="https://leerang77.github.io/blog/2025/foundation-model2/"><![CDATA[<h1 id="intro"><strong>Intro</strong></h1> <p>In Part 1 of the post, I went over motivation and intuition for fine-tuning pLMs, distinguished task-adaptive fine-tuning from domain-adaptive pretraining, introduced parameter-efficient fine-tuning, and briefly introduced Huggingface. This post will be go more in-depth on examples of fine-tuning code with Huggingface libraries. Specifically, we will cover:</p> <ol> <li><strong>Practical examples and workflow of fine-tuning code with Huggingface libraries</strong></li> <li><strong>Parameter-efficient fine-tuning with Hugginface PEFT library</strong></li> </ol> <h1 id="code-for-full-parameter-fine-tuning"><strong>Code for full parameter fine-tuning</strong></h1> <p>Below, I will show the code for four steps necessary for pLM fine-tuning, using Huggingface libraries.</p> <ol> <li>Defining the prediction head to be used with pLM</li> <li>Defining the main model that wires pLM and task model together</li> <li>Defining the data module</li> <li>Defining the optimizer and trainer</li> </ol> <h2 id="1-defining-the-prediction-head-to-be-used-with-plm"><strong>1. Defining the prediction head to be used with pLM</strong></h2> <p>For task-adaptive fine-tuning, we need a prediction head. Let’s define <code class="language-plaintext highlighter-rouge">MLPCHead</code> that can handle both residue-level and protein-level prediction tasks, and both embedding-mean and cls-token pooling strategies if protein-level prediction is used. The MLP architecture is a simple template here, and any other prediction task model (e.g. graph-based models) can be defined similarly.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">
Example prediction task model with MLP architecture
</span><span class="sh">"""</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">MLPHead</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Modular MLP head with configurable pooling method.
    Supports per-protein (mean or CLS) or per-residue classification.</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>         <span class="c1"># Should match the pLM embedding dimension
</span>        <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>        <span class="c1"># Hidden layer dimensions
</span>        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>        <span class="c1"># Number of classes (1 if regression)
</span>        <span class="n">num_hidden_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="c1"># Variable number of hidden layers in MLP
</span>        <span class="n">dropout_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>  <span class="c1"># Dropout rate
</span>        <span class="n">classification_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">protein</span><span class="sh">"</span><span class="p">,</span> <span class="c1"># 'protein' or 'residue'
</span>        <span class="n">pooling_strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="p">,</span>       <span class="c1"># 'mean' or 'cls' when protein-level
</span>    <span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Initializes the MLP prediction head.
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="c1"># Define classification mode
</span>        <span class="k">assert</span> <span class="n">classification_mode</span> <span class="ow">in</span> <span class="p">(</span>
            <span class="sh">"</span><span class="s">protein</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">residue</span><span class="sh">"</span><span class="p">,</span>
        <span class="p">),</span> <span class="sh">"</span><span class="s">classification_mode must be </span><span class="sh">'</span><span class="s">protein</span><span class="sh">'</span><span class="s"> or </span><span class="sh">'</span><span class="s">residue</span><span class="sh">'"</span>
        <span class="c1"># Define pooling strategy
</span>        <span class="k">if</span> <span class="n">classification_mode</span><span class="o">==</span><span class="sh">"</span><span class="s">protein</span><span class="sh">"</span><span class="p">:</span>
		        <span class="k">assert</span> <span class="n">pooling_strategy</span> <span class="ow">in</span> <span class="p">(</span>
		            <span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="p">,</span>
		            <span class="sh">"</span><span class="s">cls</span><span class="sh">"</span><span class="p">,</span>
		        <span class="p">),</span> <span class="sh">"</span><span class="s">pooling_strategy must be </span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="s"> or </span><span class="sh">'</span><span class="s">cls</span><span class="sh">'"</span>
        <span class="n">self</span><span class="p">.</span><span class="n">classification_mode</span> <span class="o">=</span> <span class="n">classification_mode</span>
        <span class="n">self</span><span class="p">.</span><span class="n">pooling_strategy</span> <span class="o">=</span> <span class="n">pooling_strategy</span>

        <span class="c1"># Define the architecture with the input num_hidden_layers
</span>        <span class="n">self</span><span class="p">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>
        <span class="n">dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_dim</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">hidden_dim</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_hidden_layers</span> <span class="o">+</span> <span class="p">[</span><span class="n">output_dim</span><span class="p">]</span>
        <span class="n">layers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
            <span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">))</span>
            <span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>
            <span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">())</span>
        <span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">))</span>
        <span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Forward pass for the MLP head.
        </span><span class="sh">"""</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">classification_mode</span> <span class="o">==</span> <span class="sh">"</span><span class="s">protein</span><span class="sh">"</span><span class="p">:</span> <span class="c1"># Protein-level prediction
</span>            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">pooling_strategy</span> <span class="o">==</span> <span class="sh">"</span><span class="s">cls</span><span class="sh">"</span><span class="p">:</span> <span class="c1"># BERT-style 'cls' token
</span>                <span class="n">x</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
            <span class="k">else</span><span class="p">:</span> <span class="c1"># Use mean of embeddings 
</span>                <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span> <span class="c1"># Exclude padding from the mean
</span>                    <span class="n">mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">masked</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">*</span> <span class="n">mask</span>
                    <span class="n">sum_hidden</span> <span class="o">=</span> <span class="n">masked</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">lengths</span> <span class="o">=</span> <span class="n">mask</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">x</span> <span class="o">=</span> <span class="n">sum_hidden</span> <span class="o">/</span> <span class="n">lengths</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">x</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span> <span class="c1"># Residue-level prediction
</span>            <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">.</span><span class="n">shape</span>
            <span class="n">flat</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
            <span class="n">logits_flat</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mlp</span><span class="p">(</span><span class="n">flat</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">logits_flat</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">output_dim</span><span class="p">)</span>
</code></pre></div></div> <h2 id="2-defining-the-main-model"><strong>2. Defining the main model</strong></h2> <p>Now that we have the prediction head, we need to define a model class that wires the prediction head and pLM together so that we can backpropagate through both during the supervised training. In the below example, we define a model that can be used with both protein-level and residue-level prediction, and both classification and regression tasks. As mentioned in the previous post, each of these cases require different loss functions. To help with this, let’s first define <code class="language-plaintext highlighter-rouge">TaskType</code> Enum class.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TaskType</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Enum representing different task types for the prediction head.
    </span><span class="sh">"""</span>
    <span class="n">SEQ_CLASSIFICATION</span>   <span class="o">=</span> <span class="sh">"</span><span class="s">seq_classification</span><span class="sh">"</span>
    <span class="n">SEQ_REGRESSION</span>       <span class="o">=</span> <span class="sh">"</span><span class="s">seq_regression</span><span class="sh">"</span>
    <span class="n">TOKEN_CLASSIFICATION</span> <span class="o">=</span> <span class="sh">"</span><span class="s">token_classification</span><span class="sh">"</span>
    <span class="n">TOKEN_REGRESSION</span>     <span class="o">=</span> <span class="sh">"</span><span class="s">token_regression</span><span class="sh">"</span>
</code></pre></div></div> <p>Now let’s define <code class="language-plaintext highlighter-rouge">PLMTaskModel</code> class which is our main model. It first uses <code class="language-plaintext highlighter-rouge">AutoModel</code> to load the specified pre-trained pLM and assign it to <code class="language-plaintext highlighter-rouge">self.backbone</code>. The <code class="language-plaintext highlighter-rouge">forward()</code> method first extracts embedding using <code class="language-plaintext highlighter-rouge">self.backbone</code> and then calls the prediction head using the <code class="language-plaintext highlighter-rouge">last hidden_states</code>, along with other <code class="language-plaintext highlighter-rouge">kwargs</code> required by the prediction task model. For example, if the prediciton head is a graph-based model, the graphs may be passed as additional arguments.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Any</span>

<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AutoConfig</span><span class="p">,</span>
    <span class="n">AutoModel</span><span class="p">,</span>
    <span class="n">PreTrainedModel</span><span class="p">,</span>
    <span class="n">SequenceClassifierOutput</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">PLMTaskModel</span><span class="p">(</span><span class="n">PreTrainedModel</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">General model for sequence/token classification and regression.</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">task_type</span><span class="p">:</span> <span class="n">TaskType</span><span class="p">,</span>
        <span class="n">backbone_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">head</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Initializes the PLMTaskModel with a backbone model, a head for task-specific processing,
        and an optional preprocessing function.
        
        Args:
            task_type (TaskType): Type of the task (e.g., SEQ_CLASSIFICATION, TOKEN_CLASSIFICATION).
            backbone_name (str): Name of the pretrained model backbone.
            head (nn.Module): Task-specific head to be used on top of the backbone.
            preprocess_fn (Callable[[str], str]): Function to preprocess input sequences.
        </span><span class="sh">"""</span>
        <span class="c1"># Load the config and backbone
</span>        <span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">backbone_name</span><span class="p">)</span>
        <span class="n">backbone</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">backbone_name</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
        
        <span class="c1"># Call the PretrainedModel constructor
</span>        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        
        <span class="c1"># Attach the modules
</span>        <span class="n">self</span><span class="p">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">backbone</span> <span class="c1"># Assigns the pretrained weights
</span>        <span class="n">self</span><span class="p">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">head</span> <span class="c1"># Assigns the prediction head
</span>        <span class="n">self</span><span class="p">.</span><span class="n">task_type</span> <span class="o">=</span> <span class="n">task_type</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">head_args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SequenceClassifierOutput</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Forward pass for the PLMTaskModel.
        
        Args:
            input_ids (torch.LongTensor): Input token IDs.
            attention_mask (Optional[torch.FloatTensor]): Attention mask.
            labels (Optional[torch.LongTensor]): Labels for classification tasks.
            **head_args (Any): Additional arguments for the head.
        Returns:
            SequenceClassifierOutput: Output of the model including logits and loss if labels are provided.
        </span><span class="sh">"""</span>
        <span class="c1"># Compute embedding
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">backbone</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">head</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="o">**</span><span class="n">head_args</span><span class="p">)</span>
        
        <span class="c1"># Compute loss
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">task_type</span> <span class="o">==</span> <span class="n">TaskType</span><span class="p">.</span><span class="n">TOKEN_REGRESSION</span><span class="p">:</span>
                <span class="c1"># logits: (batch, seq_len, 1) → squeeze
</span>                <span class="n">preds</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>                    <span class="c1"># (batch, seq_len)
</span>                <span class="c1"># build a mask of the real (non-pad) tokens
</span>                <span class="n">mask</span>  <span class="o">=</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">preds</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>        <span class="c1"># 1.0 for real tokens, 0.0 for pads
</span>    
                <span class="c1"># compute squared error only on real tokens
</span>                <span class="n">se</span>    <span class="o">=</span> <span class="p">(</span><span class="n">preds</span> <span class="o">-</span> <span class="n">labels</span><span class="p">.</span><span class="nf">float</span><span class="p">())</span> <span class="o">**</span> <span class="mi">2</span>         <span class="c1"># (batch, seq_len)
</span>                <span class="n">loss</span>  <span class="o">=</span> <span class="p">(</span><span class="n">se</span> <span class="o">*</span> <span class="n">mask</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">mask</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>        <span class="c1"># mean over real positions
</span>    
            <span class="k">elif</span> <span class="n">self</span><span class="p">.</span><span class="n">task_type</span> <span class="o">==</span> <span class="n">TaskType</span><span class="p">.</span><span class="n">TOKEN_CLASSIFICATION</span><span class="p">:</span>
                    <span class="c1"># logits: (batch, seq_len, num_labels)
</span>                <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">)(</span>
                    <span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
                    <span class="n">labels</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                <span class="p">)</span>
    
            <span class="k">elif</span> <span class="n">self</span><span class="p">.</span><span class="n">task_type</span> <span class="o">==</span> <span class="n">TaskType</span><span class="p">.</span><span class="n">SEQ_REGRESSION</span><span class="p">:</span>
                    <span class="c1"># logits: (batch, 1)
</span>                <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()(</span><span class="n">logits</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="nf">float</span><span class="p">())</span>
    
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># SEQ_CLASSIFICATION
</span>                    <span class="c1"># logits: (batch, num_labels)
</span>                <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()(</span>
                    <span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
                    <span class="n">labels</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                <span class="p">)</span>
     
        <span class="k">return</span> <span class="nc">SequenceClassifierOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="p">.</span><span class="n">hidden_states</span>   <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">outputs</span><span class="p">.</span><span class="n">attentions</span>         <span class="k">if</span> <span class="n">output_attentions</span>    <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></div> <p>Now we have our model. Next, we define our data module.</p> <h2 id="3-defining-the-data-module"><strong>3. Defining the data module</strong></h2> <p>We define the data module that loads, preprocesses, and tokenizes the data. To make it modular and compatible with various pLMs, we use the huggingface <code class="language-plaintext highlighter-rouge">AutoTokenizer</code> as input so that the appropriate model-specific tokenizer can be passed. It also use <code class="language-plaintext highlighter-rouge">preprocess_fn</code> as an optional input to handle any model-specific quirk (e.g. for ProtBert model, a function that adds a space between amino acids) inside the data module. We return the training, validation and optional test datasets as Huggingface <code class="language-plaintext highlighter-rouge">Dataset</code> objects, within a single <code class="language-plaintext highlighter-rouge">DatasetDict</code> object.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">List</span>

<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DatasetDict</span>

<span class="k">class</span> <span class="nc">ProteinDataModule</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Data module for protein sequence datasets, handling loading, preprocessing,
    and tokenization using Hugging Face</span><span class="sh">'</span><span class="s">s datasets library.
    This module supports training, validation, and optional test datasets.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">train_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">val_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">AutoTokenizer</span><span class="p">,</span>
        <span class="n">preprocess_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
        <span class="n">test_file</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">optional_features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>

    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DatasetDict</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Initializes the ProteinDataModule with training and validation files,
        a tokenizer, and optional preprocessing function and optional test file.
        Args:
            train_file (str): Path to the training dataset file.
            val_file (str): Path to the validation dataset file.
            tokenizer (AutoTokenizer): Tokenizer for processing sequences.
            preprocess_fn (Optional[Callable[[str], str]]): Function to preprocess sequences.
            max_length (int): Maximum length for tokenized sequences.
            test_file (Optional[str]): Path to the test dataset file, if available.
            optional_features (Optional[List[str]]): Additional features to include in the dataset.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="n">self</span><span class="p">.</span><span class="n">preprocess_fn</span> <span class="o">=</span> <span class="n">preprocess_fn</span>
        <span class="n">self</span><span class="p">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span>
        <span class="n">self</span><span class="p">.</span><span class="n">optional_features</span> <span class="o">=</span> <span class="n">optional_features</span> <span class="k">if</span> <span class="n">optional_features</span> <span class="k">else</span> <span class="p">[]</span>
        <span class="n">files</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">:</span> <span class="n">train_file</span><span class="p">,</span> <span class="sh">"</span><span class="s">validation</span><span class="sh">"</span><span class="p">:</span> <span class="n">val_file</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">test_file</span><span class="p">:</span>
            <span class="n">files</span><span class="p">[</span><span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_file</span>
        <span class="n">raw</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">csv</span><span class="sh">"</span><span class="p">,</span> <span class="n">data_files</span><span class="o">=</span><span class="n">files</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
            <span class="sh">"""</span><span class="s">
            Preprocesses the input examples by applying the preprocessing function
            and tokenizing the sequences.
            </span><span class="sh">"""</span>
            <span class="n">seqs</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="sh">"</span><span class="s">sequence</span><span class="sh">"</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">preprocess_fn</span><span class="p">:</span> <span class="c1"># Optional preprocessing step (e.g. Add space for ProtBert)
</span>                <span class="n">seqs</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="nf">preprocess_fn</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">seqs</span><span class="p">]</span>
            <span class="n">tokenized</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">tokenizer</span><span class="p">(</span>
                <span class="n">seqs</span><span class="p">,</span>
                <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">max_length</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="sh">"</span><span class="s">label</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">:</span>
                <span class="n">tokenized</span><span class="p">[</span><span class="sh">"</span><span class="s">labels</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">optional_features</span><span class="p">:</span> <span class="c1"># Optional keys for additional features (e.g. graph)
</span>                <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">:</span>
                    <span class="n">tokenized</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">tokenized</span>

        <span class="n">self</span><span class="p">.</span><span class="n">datasets</span> <span class="o">=</span> <span class="n">raw</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="n">preprocess</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_datasets</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DatasetDict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">]:</span>
        <span class="sh">"""</span><span class="s">
        Returns the processed datasets for training, validation, and optional test.
        Returns:
            DatasetDict[str, Dataset]: Dictionary containing the processed datasets.
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">datasets</span>
</code></pre></div></div> <p>Here’s an example of loading and preprocessing a dataset:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">
    Example for creating dataset to be used with ProtBert
</span><span class="sh">"""</span>

<span class="c1"># 1. Define a preprocessing function that upper-cases and spaces out residues
</span><span class="k">def</span> <span class="nf">ProtBert_preprocess</span><span class="p">(</span><span class="n">seq</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Turn a contiguous amino-acid string into uppercase
    letters separated by spaces.
    E.g. </span><span class="sh">"</span><span class="s">mkta</span><span class="sh">"</span><span class="s"> → </span><span class="sh">"</span><span class="s">M K T A</span><span class="sh">"</span><span class="s">
    </span><span class="sh">"""</span>
    <span class="n">seq</span> <span class="o">=</span> <span class="n">seq</span><span class="p">.</span><span class="nf">strip</span><span class="p">().</span><span class="nf">upper</span><span class="p">()</span>
    <span class="k">return</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">seq</span><span class="p">))</span>

<span class="c1"># 2. Load ProtBert’s tokenizer (it expects space-separated amino acids)
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">Rostlab/prot_bert</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">do_lower_case</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># 3. Instantiate your ProteinDataModule, pointing at CSVs with a "sequence" column
</span><span class="n">ProtBert_data_module</span> <span class="o">=</span> <span class="nc">ProteinDataModule</span><span class="p">(</span>
    <span class="n">train_file</span><span class="o">=</span><span class="sh">"</span><span class="s">data/train.csv</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">val_file</span><span class="o">=</span><span class="sh">"</span><span class="s">data/val.csv</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">preprocess_fn</span><span class="o">=</span><span class="n">ProtBert_preprocess</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="n">test_file</span><span class="o">=</span><span class="sh">"</span><span class="s">data/test.csv</span><span class="sh">"</span><span class="p">,</span>     <span class="c1"># optional
</span><span class="p">)</span>

<span class="c1"># 4. Get the tokenized DatasetDict
</span><span class="n">ProtBert_datasets</span><span class="p">:</span> <span class="n">DatasetDict</span> <span class="o">=</span> <span class="n">data_module</span><span class="p">.</span><span class="nf">get_datasets</span><span class="p">()</span>
</code></pre></div></div> <h2 id="4-defining-the-optimizer-and-trainer"><strong>4. Defining the optimizer and trainer</strong></h2> <p>Now that we have defined the model and the dataset, we now need to define optimizer and trainer, and optionally a scheduler for the optimizer. While we can do this with Pytorch, once again Hugginface provides a <code class="language-plaintext highlighter-rouge">Trainer</code> class that simplifies the process. The <code class="language-plaintext highlighter-rouge">Trainer</code> class in addition enables simplified workflow for distributed training and mixed-precision handling as well.</p> <p><code class="language-plaintext highlighter-rouge">Trainer</code> class actually by default implements AdamW optimizer and a linear scheduler with warmup and decay, so there’s no need to explicitly define them. It is highly customizable through the use of <code class="language-plaintext highlighter-rouge">TrainingArguments</code> class that is supplied as input to the trainer. <a href="https://huggingface.co/docs/transformers/en/main_classes/trainer">Trainer documentation</a> from Huggingface shows there are <strong>118</strong> parameters that can be passed to TrainingArguments.</p> <p>In this example, we will assume that we have written a metrics module with get_compute_metrics_fn that returns appropriate metrics function given the model task type. We use huggingface <code class="language-plaintext highlighter-rouge">DataCollatorWithPadding</code> or <code class="language-plaintext highlighter-rouge">DataCollatorForTokenClassificaiton</code> to implement per-batch dynamic padding to the length of the longest sequence in each batch. If we have We then use huggingface <code class="language-plaintext highlighter-rouge">Trainer</code> module, with <code class="language-plaintext highlighter-rouge">TrainingArguments</code> definition. By doing so, we can use pre-defined <code class="language-plaintext highlighter-rouge">trainer.train()</code> and <code class="language-plaintext highlighter-rouge">trainer.evaluate()</code> methods to simplify training.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">
Trainer Module
</span><span class="sh">"""</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AutoTokenizer</span><span class="p">,</span>
    <span class="n">Trainer</span><span class="p">,</span>
    <span class="n">TrainingArguments</span><span class="p">,</span>
    <span class="n">DataCollatorWithPadding</span><span class="p">,</span>
    <span class="n">DataCollatorForTokenClassification</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># from metrics import get_compute_metrics_fn
</span><span class="kn">from</span> <span class="n">plft.metrics</span> <span class="kn">import</span> <span class="n">get_compute_metrics_fn</span>
<span class="kn">from</span> <span class="n">plft.model</span> <span class="kn">import</span> <span class="n">PLMTaskModel</span>
<span class="kn">from</span> <span class="n">plft.config</span> <span class="kn">import</span> <span class="n">TaskType</span>

<span class="k">class</span> <span class="nc">ProteinTaskTrainer</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Trainer for protein sequence tasks, handling training and evaluation
    using Hugging Face</span><span class="sh">'</span><span class="s">s Trainer API.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">PLMTaskModel</span><span class="p">,</span>
        <span class="n">train_dataset</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">,</span>
        <span class="n">eval_dataset</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">,</span>
        <span class="n">test_dataset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dataset</span><span class="p">],</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">AutoTokenizer</span><span class="p">,</span>
        <span class="n">output_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">num_train_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">per_device_train_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Initializes the ProteinTaskTrainer with model, datasets, tokenizer, and training parameters.
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span>         <span class="o">=</span> <span class="n">model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span>
        <span class="n">self</span><span class="p">.</span><span class="n">eval_dataset</span>  <span class="o">=</span> <span class="n">eval_dataset</span>
        <span class="n">self</span><span class="p">.</span><span class="n">test_dataset</span>  <span class="o">=</span> <span class="n">test_dataset</span>

        <span class="c1"># pick the right collator:
</span>        <span class="c1"># for residue-classification, pad labels to -100 so CrossEntropyLoss(ignore_index=-100) skips them
</span>        <span class="c1"># for everything else (seq-classification, seq-/residue-regression), plain padding is sufficient
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">task_type</span> <span class="o">==</span> <span class="n">TaskType</span><span class="p">.</span><span class="n">TOKEN_CLASSIFICATION</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">data_collator</span> <span class="o">=</span> <span class="nc">DataCollatorForTokenClassification</span><span class="p">(</span>
                <span class="n">tokenizer</span><span class="p">,</span>
                <span class="n">label_pad_token_id</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">data_collator</span> <span class="o">=</span> <span class="nc">DataCollatorWithPadding</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>

        <span class="n">compute_metrics</span> <span class="o">=</span> <span class="nf">get_compute_metrics_fn</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">task_type</span><span class="p">)</span> <span class="c1"># Get the appropriate metrics function based on task type
</span>
        <span class="c1"># Initialize the Trainer
</span>        <span class="n">args</span> <span class="o">=</span> <span class="nc">TrainingArguments</span><span class="p">(</span>
            <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir</span><span class="p">,</span>
            <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">num_train_epochs</span><span class="p">,</span>
            <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">per_device_train_batch_size</span><span class="p">,</span>
            <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">per_device_train_batch_size</span><span class="p">,</span>
            <span class="n">eval_strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">save_strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">trainer</span> <span class="o">=</span> <span class="nc">Trainer</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">,</span>
            <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
            <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
            <span class="n">train_dataset</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">train_dataset</span><span class="p">,</span>
            <span class="n">eval_dataset</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">eval_dataset</span><span class="p">,</span>
            <span class="n">data_collator</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">data_collator</span><span class="p">,</span>
            <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Train the model using the huggingface Trainer module.
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">trainer</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">split</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">validation</span><span class="sh">"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sh">"""</span><span class="s">
        Evaluate the model on the specified dataset split (train, validation, or test).
        Args:
            split (str): The dataset split to evaluate on. Can be </span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">validation</span><span class="sh">"</span><span class="s">, or </span><span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="s">.
        Returns:
            Dict[str, float]: A dictionary containing evaluation metrics.
        </span><span class="sh">"""</span>
        <span class="k">if</span> <span class="n">split</span> <span class="o">==</span> <span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">:</span>
            <span class="n">ds</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">train_dataset</span>
        <span class="k">elif</span> <span class="n">split</span> <span class="o">==</span> <span class="sh">"</span><span class="s">validation</span><span class="sh">"</span><span class="p">:</span>
            <span class="n">ds</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">eval_dataset</span>
        <span class="k">elif</span> <span class="n">split</span> <span class="o">==</span> <span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">test_dataset</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">No test dataset provided.</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">ds</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">test_dataset</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Unknown split: </span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">trainer</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">ds</span><span class="p">)</span>
</code></pre></div></div> <h1 id="parameter-efficient-fine-tuning-with-peft-library"><strong>Parameter-efficient fine tuning with PEFT library</strong></h1> <p>Now that we have defined the model, datamodule, and trainer, we are almost ready for training. But there is one thing still missing: implementing parameter-efficient fine-tuning. In the previous post we briefly mentioned what it is, and that we will focus on Low Rank Adaptation (LoRA) method. Huggingface PEFT library makes the implementation of LoRA incredibly simple, so let’s look at the code first.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span>

<span class="n">base_model</span> <span class="o">=</span> <span class="nc">PLMTaskModel</span><span class="p">(...)</span> 

<span class="c1"># Create a LoRA config.
</span><span class="n">lora_config</span> <span class="o">=</span> <span class="nc">LoraConfig</span><span class="p">(</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>                         <span class="c1"># LoRA rank
</span>    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>               <span class="c1"># scaling
</span>    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">q_proj</span><span class="sh">"</span><span class="p">,</span>    <span class="c1"># which modules to inject into
</span>                    <span class="sh">"</span><span class="s">v_proj</span><span class="sh">"</span><span class="p">],</span>  
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>                <span class="c1"># optional
</span>    <span class="n">bias</span><span class="o">=</span><span class="sh">"</span><span class="s">none</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="sh">"</span><span class="s">SEQ_CLS</span><span class="sh">"</span>          <span class="c1"># one of: "SEQ_CLS", "SEQ_REG", "TOKEN_CLS", "TOKEN_REG"
</span><span class="p">)</span>

<span class="c1"># Wrap the model
</span><span class="n">peft_model</span> <span class="o">=</span> <span class="nf">get_peft_model</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="n">lora_config</span><span class="p">)</span>
</code></pre></div></div> <p>Then, use peft_model instead of base_model in the rest of the code. That’s it! This code updates the query and value projection matrices (i.e. <code class="language-plaintext highlighter-rouge">q_proj</code> and <code class="language-plaintext highlighter-rouge">v_proj</code>) using rank-8 matrices. A standard attention head computes</p> \[Q = X\,W_Q\] \[V = X\,W_V\] <p>with $W_Q, W_V\in\mathbb R^{d\times d_k}$, where $d_k$ is the dimension of attention head. With LoRA, instead of learning a full update to $W_Q$, we are introducing</p> <ul> <li>$A_Q\in\mathbb R^{d\times r}$</li> <li>$B_Q\in\mathbb R^{r\times d_k}$</li> </ul> <p>where $r \ll \min(d, d_k)$ is the rank. Using these, we modify the query and value as:</p> \[\begin{aligned} Q &amp;= X\bigl(W_Q + \tfrac{\alpha}{r}B_QA_Q^T\bigr),\\ \end{aligned}\] <p>Similarly, $W_V$ is updated as:</p> \[\begin{aligned} V &amp;= X\bigl(W_V + \tfrac{\alpha}{r}B_VA_V^T\bigr),\\ \end{aligned}\] <p>The hyperparameter $\alpha$ scales the adapter’s effect.</p> <p>During the training, only $A_Q, B_Q, A_V, B_V$ for each attention head are updated. All other weight matrices such as keys, output projections, feed-forward layers, etc. stay frozen during the training.</p>]]></content><author><name></name></author><category term="Bio-ML"/><category term="protein-language-model"/><summary type="html"><![CDATA[Practical workflow for fine-tuning with Huggingface]]></summary></entry><entry><title type="html">Fine-tuning protein language model with Huggingface (Part 1)</title><link href="https://leerang77.github.io/blog/2025/foundation-model/" rel="alternate" type="text/html" title="Fine-tuning protein language model with Huggingface (Part 1)"/><published>2025-07-23T15:09:00+00:00</published><updated>2025-07-23T15:09:00+00:00</updated><id>https://leerang77.github.io/blog/2025/foundation-model</id><content type="html" xml:base="https://leerang77.github.io/blog/2025/foundation-model/"><![CDATA[<h1 id="what-is-this-post-about"><strong>What is this post about?</strong></h1> <p>This post is a polished version of my learning about fine-tuning models on Huggingface, which I wrote some months ago. Therefore, it is likely most useful for someone with similar experience: I was working as computational biologist working with and developing ML models, had good understanding of protein language models but had primarily used them for generating embeddings rather than training them. I was not familiar with using Huggingface libraries to tune existing models and wanted to learn about it. But hopefully it will be useful for people with different backgrounds too.</p> <h2 id="what-it-covers">What it covers</h2> <ol> <li>What it means to fine-tune a protein language model and why you might want to do it (Part 1)</li> <li>What Huggingface is and how it simplifies working with pretrained transformer models (Part 1)</li> <li>Practical examples and workflow of fine-tuning code with Huggingface libraries (Part 2)</li> <li>Parameter-efficient fine-tuning with Hugginface PEFT library (Part 2)</li> </ol> <h1 id="motivation-why-fine-tune-a-protein-language-model"><strong>Motivation: Why fine tune a protein language model?</strong></h1> <p>Foundation models are becoming increasingly popular in biology across various domains. Models like <a href="https://www.evolutionaryscale.ai/blog/esm3-release">ESM series</a> and <a href="https://www.nature.com/articles/s41586-023-06139-9">Geneformer</a> have been trained on large-scale protein sequences and single-cell transcriptomics. Google Deepmind recnetly released <a href="https://research.google/blog/tx-llm-supporting-therapeutic-development-with-large-language-models/">Tx-LLM</a>, a large-language model based on PaLM2 that can predict properties across modalities (small molecules, proteins, nucleic acids, cell lines, diseases). In March, they followed up with an open-source release of <a href="https://arxiv.org/abs/2504.06196">TxGemma</a>, a smaller scale version of Tx-LLM.</p> <p>Protein language models (pLMs) like ESM pretrained on large sequence databases learn to map protein sequence to a vector embedding in a high-dimensional representation space. Thus, it becomes quantifiable how two protein sequences are similar along certain directions in the high-dimensional space while different in other dimensions. The embeddings capture the proteins’ structure, biophysical properties, and evolutionary context; different characteristics maps onto low-dimensional subspaces within the embedding.</p> <p>Therefore, these embeddings are very useful as input features for downstream property-prediction models. Suppose a supervised property-prediction model is trained on only hundreds of labeled sequences, which may seem too small for the model to generalize over the immense space of protein sequence and structure. If, however, the target property depends mainly on a few specific embedding dimensions and these labeled data adequately covers the distribution of natural proteins along those dimensions, the model could then accurately predict the property on novel sequences by examining the embedding values along those key dimensions.</p> <p><strong>If pLMs already learn to generate information-rich embeddings from pretraining on billions of sequences, why fine-tune the pLMs to a specific task?</strong> How could this help? Couldn’t it lead to overfitting and deterioriation of generizability? I recently came across a simple and elegant <a href="https://www.nature.com/articles/s41467-024-51844-2">paper</a> by Burkhard Rost Lab that tested whether fine-tuning of pLMs coupled with head prediction models help improve the performance, using three foundation models (ESM2, ProtT5, Ankh) on eight different prediction tasks. The tasks included predicting protein mutational landscapes, stability, intrinsically disordered region, sub-cellular location, and secondary structure. Nearly all of the model-task combinations showed improvement with fine-tuning of the foundation model weights!</p> <div class="row justify-content-center mt-3"> <div class="col-md-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-23-foundation-model/Rost_1-480.webp 480w,/assets/img/2025-07-23-foundation-model/Rost_1-800.webp 800w,/assets/img/2025-07-23-foundation-model/Rost_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2025-07-23-foundation-model/Rost_1.jpg" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption mt-2 text-center"> Figure 1: Reproduced from Schmirler et al. Shows the percentage differences between the fine-tuned and pretrained models for the eight prediction tasks (x-axis). Blue tiles mark statistically significant increases (&gt;1.96 standard errors; fine-tuning better). See more details at Schmirler et al. </div> </div> </div> <p><strong>What’s the physical intuition for why fine-tuning helps?</strong> When we backpropagate through both the prediction head and the foundation model during supervised training with labeled data, two things happen at once, First, the prediction head learns which embedding dimensions or their mapping correspond to the target property. Second, the encoder is refined to encode sequences in a way that sequences are most cleanly projected onto the dimensions relevant to the target property. It’s not always obvious how much extra gain we get from updating the foundation model itself versus just training a head on frozen embeddings, but in principle this joint adjustment can yield a representation that’s better aligned to the prediction task.</p> <h2 id="fine-tuning-vs-domain-adaptive-pretraining">Fine-tuning vs. Domain-adaptive pretraining</h2> <p>I have seen the term “fine-tuning” used in at least two separate contexts in literature, so as a first step, I decided to look into it more:</p> <ul> <li><strong>Scenario 1:</strong> A prediction task uses a foundation model embedding, and updates during supervised training backpropagates through both models. For example, in Schmirler et al. for predicting the mutational landscape of GFP protein, the mutant sequences are fed into ESM2 and those embeddings are fed into MLP classifier head. During the supervised training of the MLP head, the loss on mutational effect prediction is used to update the weights of both the MLP head and the ESM2 model.</li> <li><strong>Scenario 2:</strong> This scenario takes a foundation model trained on wide corpus of sequences, and continues to train with the same objective but with a more specific dataset. For example <a href="https://www.nature.com/articles/s41587-022-01618-2">Madani et al</a>, first trains ProGen model is on 281 million protein sequences from UniParc, UniProtKB, Pfam, and NCBI. ProGen is a generative model and is trained on next-token prediction problem. Then, before it’s used for generating artificial lysozyme sequences, it’s fine-tuned with 55,948-sequences that belong to phage lysozyme (PF00959), pesticin (PF16754), glucosaminidase (PF01832), glycoside hydrolase family 108 (PF05838) and transglycosylase (PF06737) from the Pfam family. Notably, in this case the objective function for fine-tuning is still the next-token prediction loss.</li> </ul> <p>These scenarios show that the term fine-tuning can be used broadly in the field. Here’s are more precise definitions for distinguishing them:</p> <ul> <li><strong>Task-adaptive fine-tuning (TaFT)</strong> <ul> <li><strong>What it is:</strong> Pretrained foundation model is attached to a downstream prediction task model, and the weights are updated using a supervised loss for the downstream property-prediction task (i.e. Scenario 1)</li> <li><strong>When to use it:</strong> When there are enough labeled examples of the target property that the foundation representations themselves should shift to better separate the examples.</li> </ul> </li> <li><strong>Domain-adaptive pretraining (DaPT)</strong> <ul> <li><strong>What it is:</strong> Pretrained foundation model continues to be trained on its original self-supervised objective (e.g. masked language modeling) but using a small, specialized dataset (i.e. Scenario 2)</li> <li><strong>When to use it:</strong> When the target proteins are from a niche family whose statistics differ substantially from the base model’s training set. The idea is to realign the foundation’s language to the domain of interest before any supervised step.</li> </ul> </li> </ul> <p>The key distinction is that domain-adaptive pre-training simply consists of training the model on its original objective. In contrast, task-adaptive fine-tuning requires connecting a foundation model with a prediction task head and backpropagating through both. <strong>For the rest of this post, we will focus on task-adaptive fine-tuning.</strong></p> <h2 id="parameter-efficient-fine-tuning">Parameter-efficient fine-tuning</h2> <p>Before diving into the implementation of fine-tuning, let’s consider its two main downsides: cost and risk.</p> <ul> <li><strong>Cost:</strong> Full fine-tuning of a foundation model with hundreds of millions to tens of billions of parameters can quickly become expensive and complicated. Training very large models will require multiple GPUs with distributed training and complex checkpointing workflows.</li> <li><strong>Risk:</strong> Updating pLM weights based on limited supervised tasks opens the door to overfitting. pLM pretraining results in embeddings that contain rich structural and physicochemical information. After the fine-tuning updates, the model may end up forgetting these more fundamental information and instead memorizing the idiosyncrasies of the small training set. This phenomenon of losing the previously learned knowledge after fine-tuning is called catastrophic forgetting.</li> </ul> <p>A strategy that can mitigate both of these problems is parameter efficient fine-tuning (PEFT). Common PEFT approaches include:</p> <ul> <li><strong>Partial fine-tuning</strong> <ul> <li>Weights from only certain parts (e.g. the final transformer block) are updated, while the others are frozen</li> </ul> </li> <li><strong>Adapters</strong> <ul> <li>Additional layers are inserted within the model (rather than on top like the prediction head) and only these are trained, while the original model weights are frozen</li> </ul> </li> <li><strong>Reparameterization (Low Rank Adaptation; LoRA)</strong> <ul> <li>Updates to weight matrices are allowed only as a product of two lower rank matrices, drastically reducing the number of trainable parameters</li> </ul> </li> </ul> <p>There are many other resources to learn more about the methods of PEFT. <a href="https://www.ibm.com/think/topics/parameter-efficient-fine-tuning">This blog post from IBM is a nice introduction.</a> In this post, I’ll mainly look at the implementation of LoRA.</p> <h1 id="implementation-of-task-adaptive-fine-tuning-with-huggingface"><strong>Implementation of task-adaptive fine-tuning with Huggingface</strong></h1> <h2 id="challenges-of-using-other-peoples-models">Challenges of using other people’s models</h2> <p>As noted before, task-adaptive fine-tuning requires connecting a pretrained foundation model with a prediction task head and backpropagating through both. Suppose we try to implement it from scratch. Because it’s hard to predict which foundation model may generate the best embedding for our downstream task (see the figure from Schmirler et al: performance of different pLMs vary for downstream tasks), we want to test several pLMs. We will face the following time-consuming and tedious tasks to get started with the pLMs:</p> <ul> <li>Installing multiple pLM packages or cloning the repos</li> <li>Creating and managing environments for each model</li> <li>Reading through documentations of varying qualities and figuring out model-specific quirks, such as handling special tokens or understanding the correct arguments for the <code class="language-plaintext highlighter-rouge">forward()</code> method</li> <li>Reading through the codes to understand the quirks if documentation isn’t great</li> </ul> <p>Adding prediction head and fine-tuning will create more headaches with:</p> <ul> <li>Having to subclass the pretrained pLM and attaching the task head</li> <li>Resolving odd dependency issues that inevitably arises</li> <li>Implementing PEFT by directly modifying the pLM architecture</li> </ul> <h2 id="standardization-of-existing-transformer-models">Standardization of existing transformer models</h2> <p>Fortunately, these problems boil down to mainly three common problems:</p> <ul> <li><strong>Setting up the model</strong> (having to install/clone, full environment or container setup for each model)</li> <li><strong>Using model-specific syntax</strong> (tokenizer, attention mask, padding, etc)</li> <li><strong>Wiring with prediction head and fine-tuning</strong></li> </ul> <p>Given that these are repeated problems, people have developed a framework that can be used to remove much of the pain from them. <strong>That is what Hugginface does</strong>.</p> <p>Huggingface helps solve these issues by providing a unified interface to access and subclass various pretrained transformer models. Through the <code class="language-plaintext highlighter-rouge">transformer</code> library we can use most of existing pLMs through a unified syntax. Let’s quickly see how it helps with each of the three pain points before we move onto practical implementation of fine-tuning in the next post.</p> <ul> <li><strong>Setting up the model → Importing using transformer library</strong> <ul> <li>Huggingface provides a single, simple API (<code class="language-plaintext highlighter-rouge">AutoModel.from_pretrained</code>) to load pretrained models and tokenizer from the Hugging Face Hub, automatically handling the model architectures and pretrained weights. There is no need to install or clone each model. For example, generating ESM2 embedding for proteins looks like this.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span> <span class="c1"># These libraries handle loading tokenizer and model from name
</span>    
  <span class="c1"># Load ESM2 tokenizer and pretrained 650M-parameter model
</span>  <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">facebook/esm2_t33_650M_UR50D</span><span class="sh">"</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">facebook/esm2_t33_650M_UR50D</span><span class="sh">"</span><span class="p">)</span>
    
  <span class="c1"># Generate embedding
</span>  <span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">MKTAYIAKQRQISFVKSHFSRQDILDLIC</span><span class="sh">"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span> <span class="c1">#Returns BatchEncoding object with input_ids and attention_mask
</span>  <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span> <span class="c1">#Returns ModelOutput class with loss, logits, hidden_states, attentions
</span>  <span class="n">embeddings</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span>  <span class="c1"># shape: [batch_size, sequence_length, hidden_dim]
</span></code></pre></div> </div> <p>If we want to use a different model, ProBert:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># Load ProtBert tokenizer and pretrained ProBert
</span>  <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">Rostlab/prot_bert</span><span class="sh">"</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">Rostlab/prot_bert</span><span class="sh">"</span><span class="p">)</span>
    
  <span class="c1"># ProtBert expects spaces between amino acids
</span>  <span class="n">sequence</span> <span class="o">=</span> <span class="sh">"</span><span class="s">MKTAYIAKQRQISFVKSHFSRQDILDLIC</span><span class="sh">"</span>
  <span class="n">sequence_spaced</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">sequence</span><span class="p">))</span> <span class="c1">#For ProBert, sequence must be formatted M K T A ...
</span>    
  <span class="c1"># Generate embedding
</span>  <span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">sequence_spaced</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
  <span class="n">embeddings</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span>
</code></pre></div> </div> <p>This second example shows that we still need to understand some quirks about the models we want to use. Here, ProtBert expects for inputs a single space between amino acids.</p> <p>Also, huggingface does not directly handle separate environments or conflicting dependency versions for each model. So if there’s a clash in libraries required by two models, this will require careful virtual environment setup. However, as long as common dependency versions for packages are compatible across multiple models, which is often the case, then multiple transformer models can be used in the same environment without needing separate isolated setups.</p> </li> <li><strong>Using model-specific syntax → Using unified syntax</strong> <ul> <li>Huggingface standardizes input/output data structures (tokenized inputs, attention masks, positional encodings and <code class="language-plaintext highlighter-rouge">ModelOutput</code> objects), so methods like <code class="language-plaintext highlighter-rouge">.encode()</code>, <code class="language-plaintext highlighter-rouge">.decode()</code>, <code class="language-plaintext highlighter-rouge">.forward()</code>, and <code class="language-plaintext highlighter-rouge">.generate()</code> work the same way across different transformer models.</li> </ul> </li> <li><strong>Wiring with prediction head and fine-tuning → Simplified by unified API</strong> <ul> <li>Huggingface provides built-in utilities (<code class="language-plaintext highlighter-rouge">Trainer</code>, <code class="language-plaintext highlighter-rouge">TrainingArguments</code>) that standardize training loops, logging, evaluation, hyperparameter tuning, and distributed training.</li> <li>The <code class="language-plaintext highlighter-rouge">PEFT</code> (for parameter-efficient fine-tuning) library provides ways to implement fine-tuning techniques like LoRA with any model from the <code class="language-plaintext highlighter-rouge">PreTrainedModel</code> class.</li> </ul> </li> </ul> <p>These second and third points will become clearer when we look at practical implementation of fine-tuning pLMs.</p> <h1 id="workflow-for-fine-tuning"><strong>Workflow for fine-tuning</strong></h1> <p>Before getting into the code, let’s conceptually break down what a full parameter pLM fine-tuning for a prediction task requires.</p> <ol> <li>Define the prediction head (classification/regression) that uses pLM embedding as input.</li> <li>Define the main model that wires the pLM and the prediction head together. Initializes the pLM with pretrained weights.</li> <li>Prepare labeled datasets for supervised training as well as validation and test.</li> <li>Define optimizer and trainer.</li> <li>Pipeline to bring everything together.</li> </ol> <p>Along the way, we will need to pay attention to correctly handling tokenization, attention mask, and padding/truncation. But Huggingface framework will help us with these too.</p> <h2 id="setting-the-goals-for-what-the-code-should-do">Setting the goals for what the code should do</h2> <p>In this post I will share the code for pLM fine-tuning. <strong>The key focus, which was most of the challenge, was to write a nice modular code that:</strong></p> <ul> <li>can be used with various pLMs as plug-and-play</li> <li>provides some template for simple prediction head, but can also work with other custom prediction head models by allowing passing of additional arguments.</li> </ul> <p>Although huggingface provides a convenient interface, there are still some model-specific quirks that made this somewhat tricky. For example, to enable various prediction heads to work as plug-and-play, the following issues must be considered.</p> <ul> <li>Models may make residue-level or protein-level prediction, and classification or regression. We need to use appropriate loss function for each case. For example: <ul> <li>Residue-level Classification <ul> <li>Example: does each residue belong to intrinsically disordered region?</li> <li>Loss function: cross entropy loss summed over residues (exclude padded residues)</li> </ul> </li> <li>Residue-level Regression <ul> <li>Example: per-residue evolutionary mutability</li> <li>Loss function: MSE loss, MAE loss, etc. summed over residues (exclude padded residues)</li> </ul> </li> <li>Protein-level Classification <ul> <li>Example: classify a protein as binder or non-binder to a given target</li> <li>Loss: cross entropy loss on sequence-level logits</li> </ul> </li> <li>Protein-level Regression <ul> <li>Example: prediction melting temperature Tm</li> <li>Loss: MSE loss, MAE loss, etc at sequence level</li> </ul> </li> </ul> </li> <li>For making a protein-level prediction, there are different ways of aggregating the embeddings across the residues. For example, some BERT-based models like ProtBert may have the special cls token that can be used for classification. User may choose to use it, or ignore that and take the mean of the embeddings across the residues.</li> </ul> <p>Moreover, the pLM themselves will also have model-specific quirks:</p> <ul> <li>Data pre-processing steps needs to be correctly handled. As pointed in Part 1, the ProtBert model requires uppercase amino acids that are separated by spaces.</li> <li>Each pLM will have certain model-specific attributes in the model architecture. For example, T5-based models like <code class="language-plaintext highlighter-rouge">ProtT5</code> has <code class="language-plaintext highlighter-rouge">self.shared</code> layer that implements vocabulary encoding. The name <code class="language-plaintext highlighter-rouge">shared</code> comes from the fact that it is a shared matrix by the encoder and decoder. Naturally, encoder-only models like <code class="language-plaintext highlighter-rouge">ESM2</code> will not have this layer. If we want a modular class for our main model that enables plug-and-play with different pLMs, we should avoid referencing specific attributes like this and only use attributes that are universal for the <code class="language-plaintext highlighter-rouge">PreTrainedModel</code> class in transformer library.</li> </ul> <p>When we look at <a href="https://github.com/RSchmirler/data-repo_plm-finetune-eval">RSchmirler et al. repo</a>, they defined separate classes for fine-tuning different pLM models (e.g. T5EncoderForSimpleSequenceClassification and load_T5_model for ProtT5, load_esm_model for ESM2, etc) and also for different tasks (e.g. T5EncoderForTokenClassificaion and T5EncoderForSimpleSequenceClassification are defined separately although most of the functionality is redundant). While this works fine for the scope of their study, it would be nice to have a more modular framework.</p> <h1 id="next-steps"><strong>Next Steps</strong></h1> <p>In the next post, I will go through the steps mentioned above for pLM fine-tuning with code examples</p>]]></content><author><name></name></author><category term="Bio-ML"/><category term="protein-language-model"/><summary type="html"><![CDATA[Motivation for fine-tuning and intro to Huggingface]]></summary></entry></feed>