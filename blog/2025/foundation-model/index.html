<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Fine-tuning protein language model with Huggingface (Part 1) | Leerang Yang </title> <meta name="author" content="Leerang Yang"> <meta name="description" content="Motivation for fine-tuning and intro to Huggingface"> <meta name="keywords" content="Leerang"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://leerang77.github.io/blog/2025/foundation-model/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Leerang</span> Yang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Fine-tuning protein language model with Huggingface (Part 1)</h1> <p class="post-meta"> Created on July 23, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/protein-language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> protein-language-model</a>   ·   <a href="/blog/category/bio-ml"> <i class="fa-solid fa-tag fa-sm"></i> Bio-ML</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="what-is-this-post-about"><strong>What is this post about?</strong></h1> <p>This post is a polished version of my learning about fine-tuning models on Huggingface, which I wrote some months ago. Therefore, it is likely most useful for someone with similar experience: I was working as computational biologist working with and developing ML models, had good understanding of protein language models but had primarily used them for generating embeddings rather than training them. I was not familiar with using Huggingface libraries to tune existing models and wanted to learn about it. But hopefully it will be useful for people with different backgrounds too.</p> <h2 id="what-it-covers">What it covers</h2> <ol> <li>What it means to fine-tune a protein language model and why you might want to do it (Part 1)</li> <li>What Huggingface is and how it simplifies working with pretrained transformer models (Part 1)</li> <li>Practical examples and workflow of fine-tuning code with Huggingface libraries (Part 2)</li> <li>Parameter-efficient fine-tuning with Hugginface PEFT library (Part 2)</li> </ol> <h1 id="motivation-why-fine-tune-a-protein-language-model"><strong>Motivation: Why fine tune a protein language model?</strong></h1> <p>Foundation models are becoming increasingly popular in biology across various domains. Models like <a href="https://www.evolutionaryscale.ai/blog/esm3-release" rel="external nofollow noopener" target="_blank">ESM series</a> and <a href="https://www.nature.com/articles/s41586-023-06139-9" rel="external nofollow noopener" target="_blank">Geneformer</a> have been trained on large-scale protein sequences and single-cell transcriptomics. Google Deepmind recnetly released <a href="https://research.google/blog/tx-llm-supporting-therapeutic-development-with-large-language-models/" rel="external nofollow noopener" target="_blank">Tx-LLM</a>, a large-language model based on PaLM2 that can predict properties across modalities (small molecules, proteins, nucleic acids, cell lines, diseases). In March, they followed up with an open-source release of <a href="https://arxiv.org/abs/2504.06196" rel="external nofollow noopener" target="_blank">TxGemma</a>, a smaller scale version of Tx-LLM.</p> <p>Protein language models (pLMs) like ESM pretrained on large sequence databases learn to map protein sequence to a vector embedding in a high-dimensional representation space. Thus, it becomes quantifiable how two protein sequences are similar along certain directions in the high-dimensional space while different in other dimensions. The embeddings capture the proteins’ structure, biophysical properties, and evolutionary context; different characteristics maps onto low-dimensional subspaces within the embedding.</p> <p>Therefore, these embeddings are very useful as input features for downstream property-prediction models. Suppose a supervised property-prediction model is trained on only hundreds of labeled sequences, which may seem too small for the model to generalize over the immense space of protein sequence and structure. If, however, the target property depends mainly on a few specific embedding dimensions and these labeled data adequately covers the distribution of natural proteins along those dimensions, the model could then accurately predict the property on novel sequences by examining the embedding values along those key dimensions.</p> <p><strong>If pLMs already learn to generate information-rich embeddings from pretraining on billions of sequences, why fine-tune the pLMs to a specific task?</strong> How could this help? Couldn’t it lead to overfitting and deterioriation of generizability? I recently came across a simple and elegant <a href="https://www.nature.com/articles/s41467-024-51844-2" rel="external nofollow noopener" target="_blank">paper</a> by Burkhard Rost Lab that tested whether fine-tuning of pLMs coupled with head prediction models help improve the performance, using three foundation models (ESM2, ProtT5, Ankh) on eight different prediction tasks. The tasks included predicting protein mutational landscapes, stability, intrinsically disordered region, sub-cellular location, and secondary structure. Nearly all of the model-task combinations showed improvement with fine-tuning of the foundation model weights!</p> <div class="row justify-content-center mt-3"> <div class="col-md-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-23-foundation-model/Rost_1-480.webp 480w,/assets/img/2025-07-23-foundation-model/Rost_1-800.webp 800w,/assets/img/2025-07-23-foundation-model/Rost_1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2025-07-23-foundation-model/Rost_1.jpg" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption mt-2 text-center"> Figure 1: Reproduced from Schmirler et al. Shows the percentage differences between the fine-tuned and pretrained models for the eight prediction tasks (x-axis). Blue tiles mark statistically significant increases (&gt;1.96 standard errors; fine-tuning better). See more details at Schmirler et al. </div> </div> </div> <p><strong>What’s the physical intuition for why fine-tuning helps?</strong> When we backpropagate through both the prediction head and the foundation model during supervised training with labeled data, two things happen at once, First, the prediction head learns which embedding dimensions or their mapping correspond to the target property. Second, the encoder is refined to encode sequences in a way that sequences are most cleanly projected onto the dimensions relevant to the target property. It’s not always obvious how much extra gain we get from updating the foundation model itself versus just training a head on frozen embeddings, but in principle this joint adjustment can yield a representation that’s better aligned to the prediction task.</p> <h2 id="fine-tuning-vs-domain-adaptive-pretraining">Fine-tuning vs. Domain-adaptive pretraining</h2> <p>I have seen the term “fine-tuning” used in at least two separate contexts in literature, so as a first step, I decided to look into it more:</p> <ul> <li> <strong>Scenario 1:</strong> A prediction task uses a foundation model embedding, and updates during supervised training backpropagates through both models. For example, in Schmirler et al. for predicting the mutational landscape of GFP protein, the mutant sequences are fed into ESM2 and those embeddings are fed into MLP classifier head. During the supervised training of the MLP head, the loss on mutational effect prediction is used to update the weights of both the MLP head and the ESM2 model.</li> <li> <strong>Scenario 2:</strong> This scenario takes a foundation model trained on wide corpus of sequences, and continues to train with the same objective but with a more specific dataset. For example <a href="https://www.nature.com/articles/s41587-022-01618-2" rel="external nofollow noopener" target="_blank">Madani et al</a>, first trains ProGen model is on 281 million protein sequences from UniParc, UniProtKB, Pfam, and NCBI. ProGen is a generative model and is trained on next-token prediction problem. Then, before it’s used for generating artificial lysozyme sequences, it’s fine-tuned with 55,948-sequences that belong to phage lysozyme (PF00959), pesticin (PF16754), glucosaminidase (PF01832), glycoside hydrolase family 108 (PF05838) and transglycosylase (PF06737) from the Pfam family. Notably, in this case the objective function for fine-tuning is still the next-token prediction loss.</li> </ul> <p>These scenarios show that the term fine-tuning can be used broadly in the field. Here’s are more precise definitions for distinguishing them:</p> <ul> <li> <strong>Task-adaptive fine-tuning (TaFT)</strong> <ul> <li> <strong>What it is:</strong> Pretrained foundation model is attached to a downstream prediction task model, and the weights are updated using a supervised loss for the downstream property-prediction task (i.e. Scenario 1)</li> <li> <strong>When to use it:</strong> When there are enough labeled examples of the target property that the foundation representations themselves should shift to better separate the examples.</li> </ul> </li> <li> <strong>Domain-adaptive pretraining (DaPT)</strong> <ul> <li> <strong>What it is:</strong> Pretrained foundation model continues to be trained on its original self-supervised objective (e.g. masked language modeling) but using a small, specialized dataset (i.e. Scenario 2)</li> <li> <strong>When to use it:</strong> When the target proteins are from a niche family whose statistics differ substantially from the base model’s training set. The idea is to realign the foundation’s language to the domain of interest before any supervised step.</li> </ul> </li> </ul> <p>The key distinction is that domain-adaptive pre-training simply consists of training the model on its original objective. In contrast, task-adaptive fine-tuning requires connecting a foundation model with a prediction task head and backpropagating through both. <strong>For the rest of this post, we will focus on task-adaptive fine-tuning.</strong></p> <h2 id="parameter-efficient-fine-tuning">Parameter-efficient fine-tuning</h2> <p>Before diving into the implementation of fine-tuning, let’s consider its two main downsides: cost and risk.</p> <ul> <li> <strong>Cost:</strong> Full fine-tuning of a foundation model with hundreds of millions to tens of billions of parameters can quickly become expensive and complicated. Training very large models will require multiple GPUs with distributed training and complex checkpointing workflows.</li> <li> <strong>Risk:</strong> Updating pLM weights based on limited supervised tasks opens the door to overfitting. pLM pretraining results in embeddings that contain rich structural and physicochemical information. After the fine-tuning updates, the model may end up forgetting these more fundamental information and instead memorizing the idiosyncrasies of the small training set. This phenomenon of losing the previously learned knowledge after fine-tuning is called catastrophic forgetting.</li> </ul> <p>A strategy that can mitigate both of these problems is parameter efficient fine-tuning (PEFT). Common PEFT approaches include:</p> <ul> <li> <strong>Partial fine-tuning</strong> <ul> <li>Weights from only certain parts (e.g. the final transformer block) are updated, while the others are frozen</li> </ul> </li> <li> <strong>Adapters</strong> <ul> <li>Additional layers are inserted within the model (rather than on top like the prediction head) and only these are trained, while the original model weights are frozen</li> </ul> </li> <li> <strong>Reparameterization (Low Rank Adaptation; LoRA)</strong> <ul> <li>Updates to weight matrices are allowed only as a product of two lower rank matrices, drastically reducing the number of trainable parameters</li> </ul> </li> </ul> <p>There are many other resources to learn more about the methods of PEFT. <a href="https://www.ibm.com/think/topics/parameter-efficient-fine-tuning" rel="external nofollow noopener" target="_blank">This blog post from IBM is a nice introduction.</a> In this post, I’ll mainly look at the implementation of LoRA.</p> <h1 id="implementation-of-task-adaptive-fine-tuning-with-huggingface"><strong>Implementation of task-adaptive fine-tuning with Huggingface</strong></h1> <h2 id="challenges-of-using-other-peoples-models">Challenges of using other people’s models</h2> <p>As noted before, task-adaptive fine-tuning requires connecting a pretrained foundation model with a prediction task head and backpropagating through both. Suppose we try to implement it from scratch. Because it’s hard to predict which foundation model may generate the best embedding for our downstream task (see the figure from Schmirler et al: performance of different pLMs vary for downstream tasks), we want to test several pLMs. We will face the following time-consuming and tedious tasks to get started with the pLMs:</p> <ul> <li>Installing multiple pLM packages or cloning the repos</li> <li>Creating and managing environments for each model</li> <li>Reading through documentations of varying qualities and figuring out model-specific quirks, such as handling special tokens or understanding the correct arguments for the <code class="language-plaintext highlighter-rouge">forward()</code> method</li> <li>Reading through the codes to understand the quirks if documentation isn’t great</li> </ul> <p>Adding prediction head and fine-tuning will create more headaches with:</p> <ul> <li>Having to subclass the pretrained pLM and attaching the task head</li> <li>Resolving odd dependency issues that inevitably arises</li> <li>Implementing PEFT by directly modifying the pLM architecture</li> </ul> <h2 id="standardization-of-existing-transformer-models">Standardization of existing transformer models</h2> <p>Fortunately, these problems boil down to mainly three common problems:</p> <ul> <li> <strong>Setting up the model</strong> (having to install/clone, full environment or container setup for each model)</li> <li> <strong>Using model-specific syntax</strong> (tokenizer, attention mask, padding, etc)</li> <li><strong>Wiring with prediction head and fine-tuning</strong></li> </ul> <p>Given that these are repeated problems, people have developed a framework that can be used to remove much of the pain from them. <strong>That is what Hugginface does</strong>.</p> <p>Huggingface helps solve these issues by providing a unified interface to access and subclass various pretrained transformer models. Through the <code class="language-plaintext highlighter-rouge">transformer</code> library we can use most of existing pLMs through a unified syntax. Let’s quickly see how it helps with each of the three pain points before we move onto practical implementation of fine-tuning in the next post.</p> <ul> <li> <strong>Setting up the model → Importing using transformer library</strong> <ul> <li>Huggingface provides a single, simple API (<code class="language-plaintext highlighter-rouge">AutoModel.from_pretrained</code>) to load pretrained models and tokenizer from the Hugging Face Hub, automatically handling the model architectures and pretrained weights. There is no need to install or clone each model. For example, generating ESM2 embedding for proteins looks like this.</li> </ul> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span> <span class="c1"># These libraries handle loading tokenizer and model from name
</span>    
  <span class="c1"># Load ESM2 tokenizer and pretrained 650M-parameter model
</span>  <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">facebook/esm2_t33_650M_UR50D</span><span class="sh">"</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">facebook/esm2_t33_650M_UR50D</span><span class="sh">"</span><span class="p">)</span>
    
  <span class="c1"># Generate embedding
</span>  <span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">MKTAYIAKQRQISFVKSHFSRQDILDLIC</span><span class="sh">"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span> <span class="c1">#Returns BatchEncoding object with input_ids and attention_mask
</span>  <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span> <span class="c1">#Returns ModelOutput class with loss, logits, hidden_states, attentions
</span>  <span class="n">embeddings</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span>  <span class="c1"># shape: [batch_size, sequence_length, hidden_dim]
</span></code></pre></div> </div> <p>If we want to use a different model, ProBert:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="c1"># Load ProtBert tokenizer and pretrained ProBert
</span>  <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">Rostlab/prot_bert</span><span class="sh">"</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">Rostlab/prot_bert</span><span class="sh">"</span><span class="p">)</span>
    
  <span class="c1"># ProtBert expects spaces between amino acids
</span>  <span class="n">sequence</span> <span class="o">=</span> <span class="sh">"</span><span class="s">MKTAYIAKQRQISFVKSHFSRQDILDLIC</span><span class="sh">"</span>
  <span class="n">sequence_spaced</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">sequence</span><span class="p">))</span> <span class="c1">#For ProBert, sequence must be formatted M K T A ...
</span>    
  <span class="c1"># Generate embedding
</span>  <span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">sequence_spaced</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">)</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
  <span class="n">embeddings</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span>
</code></pre></div> </div> <p>This second example shows that we still need to understand some quirks about the models we want to use. Here, ProtBert expects for inputs a single space between amino acids.</p> <p>Also, huggingface does not directly handle separate environments or conflicting dependency versions for each model. So if there’s a clash in libraries required by two models, this will require careful virtual environment setup. However, as long as common dependency versions for packages are compatible across multiple models, which is often the case, then multiple transformer models can be used in the same environment without needing separate isolated setups.</p> </li> <li> <strong>Using model-specific syntax → Using unified syntax</strong> <ul> <li>Huggingface standardizes input/output data structures (tokenized inputs, attention masks, positional encodings and <code class="language-plaintext highlighter-rouge">ModelOutput</code> objects), so methods like <code class="language-plaintext highlighter-rouge">.encode()</code>, <code class="language-plaintext highlighter-rouge">.decode()</code>, <code class="language-plaintext highlighter-rouge">.forward()</code>, and <code class="language-plaintext highlighter-rouge">.generate()</code> work the same way across different transformer models.</li> </ul> </li> <li> <strong>Wiring with prediction head and fine-tuning → Simplified by unified API</strong> <ul> <li>Huggingface provides built-in utilities (<code class="language-plaintext highlighter-rouge">Trainer</code>, <code class="language-plaintext highlighter-rouge">TrainingArguments</code>) that standardize training loops, logging, evaluation, hyperparameter tuning, and distributed training.</li> <li>The <code class="language-plaintext highlighter-rouge">PEFT</code> (for parameter-efficient fine-tuning) library provides ways to implement fine-tuning techniques like LoRA with any model from the <code class="language-plaintext highlighter-rouge">PreTrainedModel</code> class.</li> </ul> </li> </ul> <p>These second and third points will become clearer when we look at practical implementation of fine-tuning pLMs.</p> <h1 id="workflow-for-fine-tuning"><strong>Workflow for fine-tuning</strong></h1> <p>Before getting into the code, let’s conceptually break down what a full parameter pLM fine-tuning for a prediction task requires.</p> <ol> <li>Define the prediction head (classification/regression) that uses pLM embedding as input.</li> <li>Define the main model that wires the pLM and the prediction head together. Initializes the pLM with pretrained weights.</li> <li>Prepare labeled datasets for supervised training as well as validation and test.</li> <li>Define optimizer and trainer.</li> <li>Pipeline to bring everything together.</li> </ol> <p>Along the way, we will need to pay attention to correctly handling tokenization, attention mask, and padding/truncation. But Huggingface framework will help us with these too.</p> <h2 id="setting-the-goals-for-what-the-code-should-do">Setting the goals for what the code should do</h2> <p>In this post I will share the code for pLM fine-tuning. <strong>The key focus, which was most of the challenge, was to write a nice modular code that:</strong></p> <ul> <li>can be used with various pLMs as plug-and-play</li> <li>provides some template for simple prediction head, but can also work with other custom prediction head models by allowing passing of additional arguments.</li> </ul> <p>Although huggingface provides a convenient interface, there are still some model-specific quirks that made this somewhat tricky. For example, to enable various prediction heads to work as plug-and-play, the following issues must be considered.</p> <ul> <li>Models may make residue-level or protein-level prediction, and classification or regression. We need to use appropriate loss function for each case. For example: <ul> <li>Residue-level Classification <ul> <li>Example: does each residue belong to intrinsically disordered region?</li> <li>Loss function: cross entropy loss summed over residues (exclude padded residues)</li> </ul> </li> <li>Residue-level Regression <ul> <li>Example: per-residue evolutionary mutability</li> <li>Loss function: MSE loss, MAE loss, etc. summed over residues (exclude padded residues)</li> </ul> </li> <li>Protein-level Classification <ul> <li>Example: classify a protein as binder or non-binder to a given target</li> <li>Loss: cross entropy loss on sequence-level logits</li> </ul> </li> <li>Protein-level Regression <ul> <li>Example: prediction melting temperature Tm</li> <li>Loss: MSE loss, MAE loss, etc at sequence level</li> </ul> </li> </ul> </li> <li>For making a protein-level prediction, there are different ways of aggregating the embeddings across the residues. For example, some BERT-based models like ProtBert may have the special cls token that can be used for classification. User may choose to use it, or ignore that and take the mean of the embeddings across the residues.</li> </ul> <p>Moreover, the pLM themselves will also have model-specific quirks:</p> <ul> <li>Data pre-processing steps needs to be correctly handled. As pointed in Part 1, the ProtBert model requires uppercase amino acids that are separated by spaces.</li> <li>Each pLM will have certain model-specific attributes in the model architecture. For example, T5-based models like <code class="language-plaintext highlighter-rouge">ProtT5</code> has <code class="language-plaintext highlighter-rouge">self.shared</code> layer that implements vocabulary encoding. The name <code class="language-plaintext highlighter-rouge">shared</code> comes from the fact that it is a shared matrix by the encoder and decoder. Naturally, encoder-only models like <code class="language-plaintext highlighter-rouge">ESM2</code> will not have this layer. If we want a modular class for our main model that enables plug-and-play with different pLMs, we should avoid referencing specific attributes like this and only use attributes that are universal for the <code class="language-plaintext highlighter-rouge">PreTrainedModel</code> class in transformer library.</li> </ul> <p>When we look at <a href="https://github.com/RSchmirler/data-repo_plm-finetune-eval" rel="external nofollow noopener" target="_blank">RSchmirler et al. repo</a>, they defined separate classes for fine-tuning different pLM models (e.g. T5EncoderForSimpleSequenceClassification and load_T5_model for ProtT5, load_esm_model for ESM2, etc) and also for different tasks (e.g. T5EncoderForTokenClassificaion and T5EncoderForSimpleSequenceClassification are defined separately although most of the functionality is redundant). While this works fine for the scope of their study, it would be nice to have a more modular framework.</p> <h1 id="next-steps"><strong>Next Steps</strong></h1> <p>In the next post, I will go through the steps mentioned above for pLM fine-tuning with code examples</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Leerang Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>